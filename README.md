# Attention

A Repository of past models developed in order to understand the SOTA in attention mechanisms

Includes thus far:
 - Transformer (Canonical) [Vaswani, A. et.al.](https://arxiv.org/abs/1706.03762)
 - 